{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>Project topic:TensorFlow and Neural Networks Applications in HealthCareÿ</center></h2>\n",
    "\n",
    "<h3><center>Student Name: Liangliang Zhang</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem Statement: \n",
    "\n",
    "Use TensorFlow to build a Dense Neural Network that will be used to automatically classify fetal cardiotocogram to different fetal state (N, S, P) based on their diagnostic features data provided by the UCI Machine Learning Repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Technology\n",
    "\n",
    "TensorFlow and TensorBoard was used to built Multilayer Dense Neural Network model and monitor loss function on training dataset.  \n",
    "\n",
    "TensorFlow is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. (https://www.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descreption of  Data\n",
    "\n",
    "2126 fetal cardiotocograms (CTGs) were automatically processed and the respective diagnostic features measured. The CTGs were also classified by three expert obstetricians and a consensus classification label assigned to each of them. Classification was both with respect to a morphologic pattern (A, B, C. ...) and to a fetal state (N=normal; S=suspect; P=pathologic).\n",
    "\n",
    "URL: http://archive.ics.uci.edu/ml/machine-learning-databases/00193/\n",
    "\n",
    "Size: 1.66 MB., sample size: 2130\n",
    "\n",
    "Format of data file: .xls file of Microsoft Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardware\n",
    "\n",
    "Windows PC with Intel Core M-5Y10c CPU (0.8GHz, 998MHz) and 4GB RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sofeware\n",
    "\n",
    "Anaconda with Python 3.6.1\n",
    "\n",
    "TensorFlow 1.3.0 https://pypi.python.org/pypi/tensorflow/1.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lessons learned & Pros/Cons\n",
    "\n",
    "After tuning, my final Neural Network model gives a prediction accuracy of ~92% in training data and a prediction accuracy of ~90% in validation data.  This model performs reasonably well and I suppose that if we have more observations, especially observations of the minority class, we could have built a more powerful neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YouTube URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps & Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. load data\n",
    "\n",
    "Data fiel CTG.xls was downloaded from http://archive.ics.uci.edu/ml/machine-learning-databases/00193/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "ctg = pd.read_excel('CTG.xls', sheetname = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTG data shape:, (2130, 40)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileName</th>\n",
       "      <th>Date</th>\n",
       "      <th>SegFile</th>\n",
       "      <th>b</th>\n",
       "      <th>e</th>\n",
       "      <th>LBE</th>\n",
       "      <th>LB</th>\n",
       "      <th>AC</th>\n",
       "      <th>FM</th>\n",
       "      <th>UC</th>\n",
       "      <th>...</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>AD</th>\n",
       "      <th>DE</th>\n",
       "      <th>LD</th>\n",
       "      <th>FS</th>\n",
       "      <th>SUSP</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>NSP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Variab10.txt</td>\n",
       "      <td>1996-12-01</td>\n",
       "      <td>CTG0001.txt</td>\n",
       "      <td>240.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fmcs_1.txt</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>CTG0002.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fmcs_1.txt</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>CTG0003.txt</td>\n",
       "      <td>177.0</td>\n",
       "      <td>779.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fmcs_1.txt</td>\n",
       "      <td>1996-05-03</td>\n",
       "      <td>CTG0004.txt</td>\n",
       "      <td>411.0</td>\n",
       "      <td>1192.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       FileName       Date      SegFile      b       e    LBE     LB   AC  \\\n",
       "0           NaN        NaT          NaN    NaN     NaN    NaN    NaN  NaN   \n",
       "1  Variab10.txt 1996-12-01  CTG0001.txt  240.0   357.0  120.0  120.0  0.0   \n",
       "2    Fmcs_1.txt 1996-05-03  CTG0002.txt    5.0   632.0  132.0  132.0  4.0   \n",
       "3    Fmcs_1.txt 1996-05-03  CTG0003.txt  177.0   779.0  133.0  133.0  2.0   \n",
       "4    Fmcs_1.txt 1996-05-03  CTG0004.txt  411.0  1192.0  134.0  134.0  2.0   \n",
       "\n",
       "    FM   UC ...     C    D    E   AD   DE   LD   FS  SUSP  CLASS  NSP  \n",
       "0  NaN  NaN ...   NaN  NaN  NaN  NaN  NaN  NaN  NaN   NaN    NaN  NaN  \n",
       "1  0.0  0.0 ...   0.0  0.0  0.0  0.0  0.0  0.0  1.0   0.0    9.0  2.0  \n",
       "2  0.0  4.0 ...   0.0  0.0  0.0  1.0  0.0  0.0  0.0   0.0    6.0  1.0  \n",
       "3  0.0  5.0 ...   0.0  0.0  0.0  1.0  0.0  0.0  0.0   0.0    6.0  1.0  \n",
       "4  0.0  6.0 ...   0.0  0.0  0.0  1.0  0.0  0.0  0.0   0.0    6.0  1.0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check data shape\n",
    "print ('CTG data shape:,', ctg.shape)\n",
    "#check data head\n",
    "ctg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileName</th>\n",
       "      <th>Date</th>\n",
       "      <th>SegFile</th>\n",
       "      <th>b</th>\n",
       "      <th>e</th>\n",
       "      <th>LBE</th>\n",
       "      <th>LB</th>\n",
       "      <th>AC</th>\n",
       "      <th>FM</th>\n",
       "      <th>UC</th>\n",
       "      <th>...</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>AD</th>\n",
       "      <th>DE</th>\n",
       "      <th>LD</th>\n",
       "      <th>FS</th>\n",
       "      <th>SUSP</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>NSP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2125</th>\n",
       "      <td>S8001045.dsp</td>\n",
       "      <td>1998-06-06</td>\n",
       "      <td>CTG2127.txt</td>\n",
       "      <td>1576.0</td>\n",
       "      <td>3049.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2126</th>\n",
       "      <td>S8001045.dsp</td>\n",
       "      <td>1998-06-06</td>\n",
       "      <td>CTG2128.txt</td>\n",
       "      <td>2796.0</td>\n",
       "      <td>3415.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2127</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>564.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          FileName       Date      SegFile       b       e    LBE     LB   AC  \\\n",
       "2125  S8001045.dsp 1998-06-06  CTG2127.txt  1576.0  3049.0  140.0  140.0  1.0   \n",
       "2126  S8001045.dsp 1998-06-06  CTG2128.txt  2796.0  3415.0  142.0  142.0  1.0   \n",
       "2127           NaN        NaT          NaN     NaN     NaN    NaN    NaN  NaN   \n",
       "2128           NaN        NaT          NaN     NaN     NaN    NaN    NaN  NaN   \n",
       "2129           NaN        NaT          NaN     NaN     NaN    NaN    NaN  NaN   \n",
       "\n",
       "         FM    UC ...     C    D    E   AD   DE   LD   FS  SUSP  CLASS  NSP  \n",
       "2125    0.0   9.0 ...   0.0  0.0  1.0  0.0  0.0  0.0  0.0   0.0    5.0  2.0  \n",
       "2126    1.0   5.0 ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0    1.0  1.0  \n",
       "2127    NaN   NaN ...   NaN  NaN  NaN  NaN  NaN  NaN  NaN   NaN    NaN  NaN  \n",
       "2128    NaN   NaN ...   NaN  NaN  NaN  NaN  NaN  NaN  NaN   NaN    NaN  NaN  \n",
       "2129  564.0  23.0 ...   NaN  NaN  NaN  NaN  NaN  NaN  NaN   NaN    NaN  NaN  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check data tail\n",
    "ctg.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the column of Filename, Date and SegFile, as these information has absolutely no predictive power for determing the state of a CTG image. Keeping these information will only confuse our model when training neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctg.drop(['FileName', 'Date', 'SegFile'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop first row as it is blank, then drop a few rows from the bottom as they contain  meaningless information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctg_clear = ctg.drop(ctg.index[[0, 2127, 2128, 2129]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there are any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Having missing values? : False\n"
     ]
    }
   ],
   "source": [
    "print ('Having missing values? :', ctg_clear.isnull().any().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the shape and statistical descriptions after cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after cleaning (2126, 37)\n"
     ]
    }
   ],
   "source": [
    "print ('Data shape after cleaning', ctg_clear.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b</th>\n",
       "      <th>e</th>\n",
       "      <th>LBE</th>\n",
       "      <th>LB</th>\n",
       "      <th>AC</th>\n",
       "      <th>FM</th>\n",
       "      <th>UC</th>\n",
       "      <th>ASTV</th>\n",
       "      <th>MSTV</th>\n",
       "      <th>ALTV</th>\n",
       "      <th>...</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>AD</th>\n",
       "      <th>DE</th>\n",
       "      <th>LD</th>\n",
       "      <th>FS</th>\n",
       "      <th>SUSP</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>NSP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>240.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>43.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>177.0</td>\n",
       "      <td>779.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>411.0</td>\n",
       "      <td>1192.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>533.0</td>\n",
       "      <td>1147.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       b       e    LBE     LB   AC   FM   UC  ASTV  MSTV  ALTV ...     C  \\\n",
       "1  240.0   357.0  120.0  120.0  0.0  0.0  0.0  73.0   0.5  43.0 ...   0.0   \n",
       "2    5.0   632.0  132.0  132.0  4.0  0.0  4.0  17.0   2.1   0.0 ...   0.0   \n",
       "3  177.0   779.0  133.0  133.0  2.0  0.0  5.0  16.0   2.1   0.0 ...   0.0   \n",
       "4  411.0  1192.0  134.0  134.0  2.0  0.0  6.0  16.0   2.4   0.0 ...   0.0   \n",
       "5  533.0  1147.0  132.0  132.0  4.0  0.0  5.0  16.0   2.4   0.0 ...   0.0   \n",
       "\n",
       "     D    E   AD   DE   LD   FS  SUSP  CLASS  NSP  \n",
       "1  0.0  0.0  0.0  0.0  0.0  1.0   0.0    9.0  2.0  \n",
       "2  0.0  0.0  1.0  0.0  0.0  0.0   0.0    6.0  1.0  \n",
       "3  0.0  0.0  1.0  0.0  0.0  0.0   0.0    6.0  1.0  \n",
       "4  0.0  0.0  1.0  0.0  0.0  0.0   0.0    6.0  1.0  \n",
       "5  0.0  0.0  0.0  0.0  0.0  0.0   0.0    2.0  1.0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctg_clear.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b</th>\n",
       "      <th>e</th>\n",
       "      <th>LBE</th>\n",
       "      <th>LB</th>\n",
       "      <th>AC</th>\n",
       "      <th>FM</th>\n",
       "      <th>UC</th>\n",
       "      <th>ASTV</th>\n",
       "      <th>MSTV</th>\n",
       "      <th>ALTV</th>\n",
       "      <th>...</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>AD</th>\n",
       "      <th>DE</th>\n",
       "      <th>LD</th>\n",
       "      <th>FS</th>\n",
       "      <th>SUSP</th>\n",
       "      <th>CLASS</th>\n",
       "      <th>NSP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "      <td>2126.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>878.439793</td>\n",
       "      <td>1702.877234</td>\n",
       "      <td>133.303857</td>\n",
       "      <td>133.303857</td>\n",
       "      <td>2.722484</td>\n",
       "      <td>7.241298</td>\n",
       "      <td>3.659925</td>\n",
       "      <td>46.990122</td>\n",
       "      <td>1.332785</td>\n",
       "      <td>9.84666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024929</td>\n",
       "      <td>0.038100</td>\n",
       "      <td>0.033866</td>\n",
       "      <td>0.156162</td>\n",
       "      <td>0.118532</td>\n",
       "      <td>0.050329</td>\n",
       "      <td>0.032455</td>\n",
       "      <td>0.092662</td>\n",
       "      <td>4.509878</td>\n",
       "      <td>1.304327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>894.084748</td>\n",
       "      <td>930.919143</td>\n",
       "      <td>9.840844</td>\n",
       "      <td>9.840844</td>\n",
       "      <td>3.560850</td>\n",
       "      <td>37.125309</td>\n",
       "      <td>2.847094</td>\n",
       "      <td>17.192814</td>\n",
       "      <td>0.883241</td>\n",
       "      <td>18.39688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155947</td>\n",
       "      <td>0.191482</td>\n",
       "      <td>0.180928</td>\n",
       "      <td>0.363094</td>\n",
       "      <td>0.323314</td>\n",
       "      <td>0.218675</td>\n",
       "      <td>0.177248</td>\n",
       "      <td>0.290027</td>\n",
       "      <td>3.026883</td>\n",
       "      <td>0.614377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>287.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>1009.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>538.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1521.000000</td>\n",
       "      <td>2434.750000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>11.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3296.000000</td>\n",
       "      <td>3599.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>91.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 b            e          LBE           LB           AC  \\\n",
       "count  2126.000000  2126.000000  2126.000000  2126.000000  2126.000000   \n",
       "mean    878.439793  1702.877234   133.303857   133.303857     2.722484   \n",
       "std     894.084748   930.919143     9.840844     9.840844     3.560850   \n",
       "min       0.000000   287.000000   106.000000   106.000000     0.000000   \n",
       "25%      55.000000  1009.000000   126.000000   126.000000     0.000000   \n",
       "50%     538.000000  1241.000000   133.000000   133.000000     1.000000   \n",
       "75%    1521.000000  2434.750000   140.000000   140.000000     4.000000   \n",
       "max    3296.000000  3599.000000   160.000000   160.000000    26.000000   \n",
       "\n",
       "                FM           UC         ASTV         MSTV        ALTV  \\\n",
       "count  2126.000000  2126.000000  2126.000000  2126.000000  2126.00000   \n",
       "mean      7.241298     3.659925    46.990122     1.332785     9.84666   \n",
       "std      37.125309     2.847094    17.192814     0.883241    18.39688   \n",
       "min       0.000000     0.000000    12.000000     0.200000     0.00000   \n",
       "25%       0.000000     1.000000    32.000000     0.700000     0.00000   \n",
       "50%       0.000000     3.000000    49.000000     1.200000     0.00000   \n",
       "75%       2.000000     5.000000    61.000000     1.700000    11.00000   \n",
       "max     564.000000    23.000000    87.000000     7.000000    91.00000   \n",
       "\n",
       "          ...                 C            D            E           AD  \\\n",
       "count     ...       2126.000000  2126.000000  2126.000000  2126.000000   \n",
       "mean      ...          0.024929     0.038100     0.033866     0.156162   \n",
       "std       ...          0.155947     0.191482     0.180928     0.363094   \n",
       "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "50%       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "75%       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "max       ...          1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "                DE           LD           FS         SUSP        CLASS  \\\n",
       "count  2126.000000  2126.000000  2126.000000  2126.000000  2126.000000   \n",
       "mean      0.118532     0.050329     0.032455     0.092662     4.509878   \n",
       "std       0.323314     0.218675     0.177248     0.290027     3.026883   \n",
       "min       0.000000     0.000000     0.000000     0.000000     1.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     2.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     4.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     7.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000    10.000000   \n",
       "\n",
       "               NSP  \n",
       "count  2126.000000  \n",
       "mean      1.304327  \n",
       "std       0.614377  \n",
       "min       1.000000  \n",
       "25%       1.000000  \n",
       "50%       1.000000  \n",
       "75%       1.000000  \n",
       "max       3.000000  \n",
       "\n",
       "[8 rows x 37 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctg_clear.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Extract feature and lables\n",
    "\n",
    "The dataset has two types of labels: morphologic pattern and fetal state. In this project, we only use fetal state label to perform a 3-class classification. Then fetal labels was then onehot encoded to dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of abservations: 2126\n",
      "Number of features: 25\n",
      "number of labels: 3\n"
     ]
    }
   ],
   "source": [
    "features = ctg_clear.iloc[:, :-12].values\n",
    "labels = ctg_clear.iloc[:, -1].values\n",
    "\n",
    "labels_onehot = pd.get_dummies(labels)\n",
    "\n",
    "print ('Number of abservations:', features.shape[0])\n",
    "print ('Number of features:', features.shape[1])\n",
    "print ('number of labels:', labels_onehot.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Train and validation data split\n",
    "\n",
    "Then entire dataset was randomly split into training (80% 1700 cases) and validation (20% 426 cases) dataset. Train dataset is used for training our neural network, and validation datasetis used for testing the accurarcy of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data/label shape:  (1700, 25) (1700, 3)\n",
      "Validation data/label shape:  (426, 25) (426, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Take 1/5 images from the training data, and leave the remainder in training\n",
    "train_dataset, valid_dataset, train_labels, valid_labels = train_test_split(features, labels_onehot.values, test_size=0.2, random_state=10)\n",
    "print('Training data/label shape: ', train_dataset.shape, train_labels.shape)\n",
    "print('Validation data/label shape: ', valid_dataset.shape, valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propotion for each class in train data: [ 0.77588235  0.13764706  0.08647059]\n",
      "Propotion for each class in validaion data: [ 0.78873239  0.14319249  0.06807512]\n"
     ]
    }
   ],
   "source": [
    "#check the propotion of each class in train and validation data\n",
    "print ('Propotion for each class in train data:', np.sum(train_labels, axis=0)/train_labels.shape[0])\n",
    "print ('Propotion for each class in validaion data:', np.sum(valid_labels, axis=0)/valid_labels.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The propotion for each class is similar in training and validation dataset, so we will have all information needed in training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Dense Neural Network (DNN) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Define a few useful  functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate accuracy by identifying validation cases where the model's highest-probability class matches the true y label \n",
    "def accuracy(predictions, labels):\n",
    "    correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(labels, 1))\n",
    "    accuracy_pct = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) * 100.0\n",
    "    #another way to calculate this is to use np like following\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n",
    "    #return accuracy_pct.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape, name):\n",
    "    initial = tf.truncated_normal(shape, stddev=1e-4)\n",
    "    #initial = tf.truncated_normal(shape, stddev=np.sqrt(2.0/shape[0]))\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def bias_variable(shape, name):\n",
    "    #initial = tf.constant(0.1, shape=shape)\n",
    "    initial = tf.zeros(shape)\n",
    "    return tf.Variable(initial, name)\n",
    "\n",
    "split_by_half = lambda x,k : int(x/2**k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Simple 2-layer DNN model with GradientDescentOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_dataset = valid_dataset.astype(np.float32)\n",
    "n_labels = 3\n",
    "batch_size = 99\n",
    "flattened_size = train_dataset.shape[1]\n",
    "hidden_nodes = 100\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, flattened_size), name=\"TrainingData\")\n",
    "    tf_train_labelset = tf.placeholder(tf.float32, shape=(batch_size, n_labels), name=\"TrainingLabels\")\n",
    "    tf_valid_dataset = tf.constant(valid_dataset, name=\"ValidationData\")\n",
    "    \n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([flattened_size, hidden_nodes]), name=\"weights1\")\n",
    "    layer1_biases = tf.Variable(tf.zeros([hidden_nodes]), name=\"biases1\")\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([hidden_nodes, n_labels]), name=\"weights2\")\n",
    "    layer2_biases = tf.Variable(tf.ones([n_labels]), name=\"biases2\")\n",
    "\n",
    "    # Model.\n",
    "    def model(data, name):\n",
    "        with tf.name_scope(name) as scope:\n",
    "            layer1 = tf.add(tf.matmul(data, layer1_weights), layer1_biases, name=\"layer1\")\n",
    "            hidden1 = tf.nn.relu(layer1, name=\"relu1\")\n",
    "            layer2 = tf.add(tf.matmul(hidden1, layer2_weights), layer2_biases, name=\"layer2\")\n",
    "            return layer2 \n",
    "    \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset, name=\"logits\")\n",
    "    #loss function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels = tf_train_labelset), name=\"loss\")\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset, name=\"validation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define run model function\n",
    "def run_session(num_epochs, name):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run() \n",
    "        merged = tf.summary.merge_all()  \n",
    "        writer = tf.summary.FileWriter(\"tmp/tensorflowlogs\", session.graph)\n",
    "        print(\"Initialized model:\", name)\n",
    "        for epoch in range(num_epochs):\n",
    "            offset = (epoch * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labelset : batch_labels}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            if (epoch % 500 == 0):\n",
    "                print('Minibatch loss at epoch %d: %f' % (epoch, l))\n",
    "                print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "                print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized model: DNN_2layer\n",
      "Minibatch loss at epoch 0: 3036.268311\n",
      "Minibatch accuracy: 81.8%\n",
      "Validation accuracy: 12.4%\n",
      "Minibatch loss at epoch 500: 0.675400\n",
      "Minibatch accuracy: 77.8%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at epoch 1000: 0.566603\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at epoch 1500: 0.635244\n",
      "Minibatch accuracy: 79.8%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at epoch 2000: 0.715524\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at epoch 2500: 0.618097\n",
      "Minibatch accuracy: 80.8%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at epoch 3000: 0.715514\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at epoch 3500: 0.629444\n",
      "Minibatch accuracy: 79.8%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at epoch 4000: 0.606569\n",
      "Minibatch accuracy: 80.8%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at epoch 4500: 0.738649\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at epoch 5000: 0.687034\n",
      "Minibatch accuracy: 77.8%\n",
      "Validation accuracy: 78.6%\n"
     ]
    }
   ],
   "source": [
    "run_session(5001, \"DNN_2layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After 5000 epoches, both training and validation accuracies are arround 76%, which is similar to a blind guess of first class.\n",
    "\n",
    "Next, we modify several parameters of our DNN model to see if we can improve model performance. Modifications are listed below:\n",
    "1.\tMore hidden layers\n",
    "2.\tRegularization and dropout to avoid over fitting\n",
    "3.\tAltinative optimizer\n",
    "\n",
    "Also, summary for loss function, train accuracy and validation accuracy were added to TensroBoard, so we can keep tracking our model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 4-layer DNN model with regularization, dropout and  AdamOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "flattened_size = train_dataset.shape[1]\n",
    "hidden_nodes = 1024\n",
    "lamb_reg = 0.01\n",
    "learning_rate = 0.001  #  learning rate for the momentum optimizer\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, flattened_size), name=\"TrainingData\")\n",
    "    tf_train_labelset = tf.placeholder(tf.float32, shape=(batch_size, n_labels), name=\"TrainingLabels\")\n",
    "    tf_valid_dataset = tf.constant(valid_dataset, name=\"ValidationData\")\n",
    "    tf_valid_labelset = tf.constant(valid_labels, name=\"ValidationLabels\")\n",
    "    # Variables.\n",
    "    layer1_weights = weight_variable([flattened_size, hidden_nodes], name=\"weights1\")\n",
    "    layer1_biases = bias_variable([hidden_nodes], name=\"biases1\")\n",
    "    layer2_weights = weight_variable([hidden_nodes, split_by_half(hidden_nodes,1)], name=\"weights2\")\n",
    "    layer2_biases = bias_variable([split_by_half(hidden_nodes,1)], name=\"biases2\")\n",
    "    layer3_weights = weight_variable([split_by_half(hidden_nodes,1), split_by_half(hidden_nodes,2)], name=\"weights3\")\n",
    "    layer3_biases = bias_variable([split_by_half(hidden_nodes,2)], name=\"biases3\")\n",
    "    layer4_weights = weight_variable([split_by_half(hidden_nodes,2), n_labels], name=\"weights4\")\n",
    "    layer4_biases = bias_variable([n_labels], name=\"biases4\")\n",
    "        \n",
    "    keep_prob = tf.placeholder(\"float\", name=\"keep_prob\")\n",
    "    \n",
    "    def model(data, name, proba=keep_prob):\n",
    "        with tf.name_scope(name) as scope:\n",
    "            layer1 = tf.add(tf.matmul(data, layer1_weights), layer1_biases, name=\"layer1\")\n",
    "            hidden1 = tf.nn.dropout(tf.nn.relu(layer1), proba, name=\"dropout1\")   # dropout on the hidden layer\n",
    "            layer2 = tf.add(tf.matmul(hidden1, layer2_weights), layer2_biases, name=\"layer2\")  # a new hidden layer\n",
    "            hidden2 = tf.nn.dropout(tf.nn.relu(layer2), proba, name=\"dropout2\")\n",
    "            layer3 = tf.add(tf.matmul(hidden2, layer3_weights), layer3_biases, name=\"layer3\")\n",
    "            hidden3 = tf.nn.dropout(tf.nn.relu(layer3), proba)\n",
    "            layer4 = tf.add(tf.matmul(hidden3, layer4_weights), layer4_biases, name=\"layer4\")\n",
    "            return layer4\n",
    "    \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset, \"logits\", keep_prob)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels = tf_train_labelset), name=\"loss\")\n",
    "    regularizers = (tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer1_biases) +\n",
    "                    tf.nn.l2_loss(layer2_weights) + tf.nn.l2_loss(layer2_biases) +\n",
    "                    tf.nn.l2_loss(layer3_weights) + tf.nn.l2_loss(layer3_biases) +\n",
    "                    tf.nn.l2_loss(layer4_weights) + tf.nn.l2_loss(layer4_biases) )\n",
    "\n",
    "    # Add the regularization term to the loss.\n",
    "    loss += lamb_reg * regularizers\n",
    "    #loss = tf.reduce_mean(loss + lamb_reg * regularizers)\n",
    "    \n",
    "    # Optimizer\n",
    "    #global_step = tf.Variable(0, name=\"globalstep\")  # count  number of steps taken.\n",
    "    #optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9).minimize(loss, global_step=global_step)\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset, \"validation\", 1.0))  # no dropout\n",
    "    #saver = tf.train.Saver()   # a saver variable to save the model\n",
    "    \n",
    "    # acuuracy for training data\n",
    "    train_correct_prediction = tf.equal(tf.cast(tf.argmax(logits, 1), tf.float32), (tf.cast(tf.argmax(tf_train_labelset, 1), tf.float32)))\n",
    "    accuracy_train = tf.reduce_mean(tf.cast(train_correct_prediction, tf.float32))\n",
    "    # acuuracy for validation data\n",
    "    valid_correct_prediction = tf.equal(tf.cast(tf.argmax(model(tf_valid_dataset, \"validation\", 1.0), 1), tf.float32), (tf.cast(tf.argmax(tf_valid_labelset, 1), tf.float32)))\n",
    "    accuracy_valid = tf.reduce_mean(tf.cast(valid_correct_prediction, tf.float32))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_session_2(num_epochs, name, k_prob=1.0):\n",
    "\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run() \n",
    "        \n",
    "        # summaries\n",
    "        loss_summary = tf.summary.scalar('Loss', loss)\n",
    "        \n",
    "        train_accuracy_summary = tf.summary.scalar('train_accuracy', accuracy_train)\n",
    "        valid_accuracy_summary = tf.summary.scalar('valid_accuracy', accuracy_valid)\n",
    "        \n",
    "        merged = tf.summary.merge_all()  \n",
    "        writer = tf.summary.FileWriter(\"tmp/tensorflowlogs_5\", session.graph)\n",
    "        \n",
    "        print('Initialized model:', name,\"\\n\")\n",
    "        for epoch in range(num_epochs):\n",
    "            offset = (epoch * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labelset : batch_labels, keep_prob : k_prob}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            writer.add_summary(loss_summary.eval(feed_dict=feed_dict), epoch)\n",
    "            writer.add_summary(train_accuracy_summary.eval(feed_dict=feed_dict), epoch)\n",
    "            writer.add_summary(valid_accuracy_summary.eval(feed_dict=feed_dict), epoch)\n",
    "            #writer.add_summary(learning_rate_summary.eval(), epoch)\n",
    "            if (epoch % 500 == 0):\n",
    "                print(\"Minibatch loss at epoch {}: {}\".format(epoch, l))\n",
    "                print(\"Minibatch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n",
    "                print(\"Validation accuracy: {:.1f}\\n\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
    "        #save_path = saver.save(session, \"tmp/\" + name +\".ckpt\")\n",
    "        #print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized model: DNN_4layer_Adam \n",
      "\n",
      "Minibatch loss at epoch 0: 1.0986384153366089\n",
      "Minibatch accuracy: 77.0\n",
      "Validation accuracy: 78.9\n",
      "\n",
      "Minibatch loss at epoch 500: 0.4013174772262573\n",
      "Minibatch accuracy: 89.0\n",
      "Validation accuracy: 83.1\n",
      "\n",
      "Minibatch loss at epoch 1000: 0.45314696431159973\n",
      "Minibatch accuracy: 83.0\n",
      "Validation accuracy: 83.6\n",
      "\n",
      "Minibatch loss at epoch 1500: 0.32736605405807495\n",
      "Minibatch accuracy: 90.0\n",
      "Validation accuracy: 88.3\n",
      "\n",
      "Minibatch loss at epoch 2000: 0.2966890037059784\n",
      "Minibatch accuracy: 90.0\n",
      "Validation accuracy: 89.0\n",
      "\n",
      "Minibatch loss at epoch 2500: 0.25094926357269287\n",
      "Minibatch accuracy: 93.0\n",
      "Validation accuracy: 85.9\n",
      "\n",
      "Minibatch loss at epoch 3000: 0.35042813420295715\n",
      "Minibatch accuracy: 86.0\n",
      "Validation accuracy: 88.7\n",
      "\n",
      "Minibatch loss at epoch 3500: 0.2360188364982605\n",
      "Minibatch accuracy: 93.0\n",
      "Validation accuracy: 88.7\n",
      "\n",
      "Minibatch loss at epoch 4000: 0.2891061007976532\n",
      "Minibatch accuracy: 91.0\n",
      "Validation accuracy: 89.7\n",
      "\n",
      "Minibatch loss at epoch 4500: 0.2032267153263092\n",
      "Minibatch accuracy: 95.0\n",
      "Validation accuracy: 88.7\n",
      "\n",
      "Minibatch loss at epoch 5000: 0.27774709463119507\n",
      "Minibatch accuracy: 89.0\n",
      "Validation accuracy: 89.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_session_2(5001, \"DNN_4layer_Adam\", 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified Neural Network model gives a prediction accuracy of ~92%  in training data and a prediction accuracy of ~90%  in validation data.\n",
    "\n",
    "Different optimizer (MomentumOptimizer, AdamOptimizer, GradientDescentOptimizer), learning rate (0.001, 0.01, 0.1) and keep probability (1.0, 0.8, 0.5) were tested. The final DNN model (AdamOptimizer, learning rate =0.01, keep probability=1.0), which has the best performance on validation data, was shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"roulette.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 6. Conclusion\n",
    "\n",
    "Our final Neural Network model gives a prediction accuracy of ~92% in training data and a prediction accuracy of ~90% in validation data. This model performs reasonably well and I suppose that if we have more observations, especially observations of the minority class, we could have built a more powerful neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
